{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open(\"../jupyter/69M_reddit_accounts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "\n",
    "for line in r.readlines():\n",
    "    \n",
    "    #if len(line_fragment) == 1:    \n",
    "    count +=1\n",
    "    if count > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_object, chunk_size=1024):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece.\n",
    "    Default chunk size: 1k.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "\n",
    "# create a fil object for a very large file, such as ...\n",
    "f = open(\"../jupyter/69M_reddit_accounts.csv\")\n",
    "\n",
    "# list for all csv lines ... this will be a list of lists so it's easier to chunk\n",
    "all_lines = []\n",
    "\n",
    "# list for storing partial lines from chunks\n",
    "line_fragment = []\n",
    "\n",
    "for piece in read_in_chunks(f):\n",
    "    #look for old line fragment and add to piece\n",
    "    if len(line_fragment) > 0:\n",
    "        new_piece = line_fragment[0] + piece\n",
    "        # empty the list\n",
    "        line_fragment = []\n",
    "    else:\n",
    "        new_piece = piece\n",
    "        \n",
    "    mylines = new_piece.split(\"\\n\")\n",
    "    \n",
    "    # isolate last item, which will be a line fragment\n",
    "    last_item = mylines[-1]\n",
    "    \n",
    "    # remove last_item from mylines\n",
    "    mylines_without_tail = mylines[:-1]\n",
    "    \n",
    "    # set line_fragment to new fragment\n",
    "    line_fragment.append(last_item)\n",
    "    \n",
    "    # append mylines to all_lines \n",
    "    all_lines.append(mylines_without_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['164491063593,riipbdo,1535646541,1536828889,0,0',\n",
       " '164491541233,varma01,1535646604,1536828889,0,0',\n",
       " '164491671272,MarklarOG,1535646645,1536828889,0,2',\n",
       " '164491914968,nergdnul,1535646674,1536828889,1,0',\n",
       " '164492207199,xoalexa78,1535646688,1536828889,0,11',\n",
       " '164492296916,Feeakonbrownies,1535646700,1536828889,0,0',\n",
       " '164492379842,harinderkohli,1535646709,1536828889,3,50',\n",
       " '164492590690,Some1s_Mother,1535646734,1536828889,0,3',\n",
       " '164492661914,Preppeh,1535646743,1536828889,0,0',\n",
       " '164492690921,dinklesquid,1535646746,1536828889,0,0',\n",
       " '164492986785,YogaMcGee,1535646783,1536828889,0,0',\n",
       " '164493619326,Formyisaiah,1535646862,1536828889,0,9',\n",
       " '164493856395,Att_throwaway_NJ,1535646890,1536828889,1,2',\n",
       " '164493914016,annaylin,1535646897,1536828889,0,0',\n",
       " '164494011381,redbull6,1535646965,1536828889,0,0',\n",
       " '164494252493,PuzzledPeee,1535646941,1536828889,0,10',\n",
       " '164496167832,lkasdfalksdfjklj,1535647182,1536828889,0,0']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now all_lines is a list of csv chunks\n",
    "# each chunk begins at the start of a line and ends at the end of a line\n",
    "all_lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,name,created_utc,updated_on,comment_karma,link_karma'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the header is in the first position in the first csv chunk\n",
    "header = all_lines[0][0]\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this all one big list, I could use a loop, but this will still take a very long time\n",
    "# this is a lot of data!\n",
    "all_data_flat = []\n",
    "for chunk in all_lines:\n",
    "    for row in chunk:\n",
    "        split_row = row.split(\",\")\n",
    "        all_data_flat.append(split_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69382539"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,name,created_utc,updated_on,comment_karma,link_karma'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's worse, all_data_flat is now so big that it would be hard to do anything with it\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(all_data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what can I do? \n",
    "# My best option is probably to store the data in a database or index and work with rows in smaller chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
